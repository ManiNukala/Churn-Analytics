---
title: "Churn_analyticst"
output: rmarkdown::github_document
---
```{r, echo=FALSE}
knitr::opts_chunk$set(
  fig.path = "README_figs/README-"
)
```
```

## R Markdown
---
In this project we will be doing exploratory data analysis and build high performance models to predict customer churn.
Let us start off by importing required libraries and reading in our data.
---
```{r}
library(tidyverse)
library(tidyquant)
library(rsample)
library(recipes)
library(corrr)
library(reshape2)
library(ggplot2)
library(Amelia)
library(ggpubr)
library(caret)
library(ROCR)
library(randomForest)
library(mlbench)
library(caret)
library(DMwR)
data=read.csv(file.choose())
```
---
Initial glimpse
---

```{r, echo=TRUE}
dim(data)
names(data)
```

```{r, echo=TRUE}
head(data,5)
```


```{r, echo=TRUE}
sapply(data,class)
```


```{r, echo=TRUE}
missmap(data)
```

---
Looks like there are very few missing values in the total purchases column. Lets start pre-processing the data by removing rows with these values and the customer ID column. We will also convert senior citizen and character columns to factor.
---

```{r, echo=TRUE}
data = data %>%
  select(-customerID) %>%
  drop_na() %>%
  select(Churn, everything())

data = data %>% mutate_if(is.character, as.factor)
data$SeniorCitizen = as.factor(data$SeniorCitizen) 

```
---
# Exploratory Data Analysis
We are ready to start exploring. Lets see how various columns play a role in churn rates. We'll start with gender.
---
```{r, echo=TRUE}
data %>%
  group_by(gender) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n))
```


```{r, echo=TRUE}
data %>%
  group_by(gender, Churn) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n))
```


---
It doesnt look like gender plays a meaningful role. The churn rates are more or less the same. Let us now have a look at senior citizen variable.
---

```{r, echo=TRUE}
data %>%
  group_by(SeniorCitizen) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n))
```

```{r, echo=TRUE}
data %>%
  group_by(SeniorCitizen, Churn) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n))
```

---
Out of the 16% of the customers who are senior citizens almost 42% churned. 24% churn rate can be observed in the remaining population. We can infer than senior citizens are more likely to churn.
---

```{r, echo=TRUE}
data %>%
  group_by(Partner) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n))
```

```{r, echo=TRUE}
data %>%
  group_by(Partner, Churn) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n))
```

---
People with partners have 20% churn rate. On the other hand 33% churn rate can be observed among people without partners.
---
```{r, echo=TRUE}
data %>%
  group_by(Dependents) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n))
```

```{r, echo=TRUE}
data %>%
  group_by(Dependents, Churn) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n))
```

---
15% churn rate can be observed among people who have dependents. Among those who do not have dependants, we can see a 30% churn rate. It appears that people who do not have dependents are twice as likely to churn.

Among the variables we observed earlier, we consider subsets where higher churn rate is observed i.e, senior citizen, people without dependants and people without partners. Let us observed the total charges of customers in these segments who have churned.
---
```{r, echo=TRUE}
data %>%
  select(SeniorCitizen, Churn, TotalCharges, tenure) %>%
  filter(SeniorCitizen == 1, Churn == "Yes") %>%
  summarize(n = n(),
            total = sum(TotalCharges),
            avg_tenure = sum(tenure)/n)
```


```{r, echo=TRUE}
data %>%
  select(Partner, Churn, TotalCharges, tenure) %>%
  filter(Partner == "No", Churn == "Yes") %>%
  summarize(n = n(),
            total = sum(TotalCharges),
            avg_tenure = sum(tenure)/n)
```


```{r, echo=TRUE}
data %>%
  select(Dependents, Churn, TotalCharges, tenure) %>%
  filter(Dependents == "No", Churn == "Yes") %>%
  summarize(n = n(),
            total = sum(TotalCharges),
            avg_tenure = sum(tenure)/n)
```

---
Among the segments, the highest loss of 2.3 million USD can be observed in people without dependents. People without partners churned 1.3 million USD while senior citizens churned 0.9 million USD.

Let us observed the services used by the segment of people without dependents
---
```{r}
dependents = data %>% filter(Dependents == "No")
p1=ggplot(dependents,aes(x=PhoneService,fill=Churn))+geom_bar()+geom_text(stat='count',aes(label=..count..))
p2=ggplot(dependents,aes(x=MultipleLines,fill=Churn))+geom_bar()+geom_text(stat='count',aes(label=..count..))
p3=ggplot(dependents,aes(x=InternetService,fill=Churn))+geom_bar()+geom_text(stat='count',aes(label=..count..))
p4=ggplot(dependents,aes(x=OnlineSecurity,fill=Churn))+geom_bar()+geom_text(stat='count',aes(label=..count..))
p5=ggplot(dependents,aes(x=OnlineBackup,fill=Churn))+geom_bar()+geom_text(stat='count',aes(label=..count..))
p6=ggplot(dependents,aes(x=DeviceProtection,fill=Churn))+geom_bar()+geom_text(stat='count',aes(label=..count..))
p7=ggplot(dependents,aes(x=TechSupport,fill=Churn))+geom_bar()+geom_text(stat='count',aes(label=..count..))
p8=ggplot(dependents,aes(x=StreamingTV,fill=Churn))+geom_bar()+geom_text(stat='count',aes(label=..count..))
p9=ggplot(dependents,aes(x=StreamingMovies,fill=Churn))+geom_bar()+geom_text(stat='count',aes(label=..count..))
p10=ggplot(dependents,aes(x=Contract,fill=Churn))+geom_bar()+geom_text(stat='count',aes(label=..count..))
p11=ggplot(dependents,aes(x=PaperlessBilling,fill=Churn))+geom_bar()+geom_text(stat='count',aes(label=..count..))

```


```{r, echo=TRUE}
ggarrange(p1,p2,p3,p4, ncol=2,nrow=2)
```


```{r, echo=TRUE}
ggarrange(p5, p6 , p7 ,p8, ncol=2,nrow=2)
```


```{r, echo=TRUE}
ggarrange(p9,p10,p11, ncol=2,nrow=2)
```

---
High churn rates can be observed in people using phone service. Maybe they do not use the service often. Removing phone service from their plan might mitigate their expenses and improve retention.

People who use fiber optic interned have churned significantly. Perhaps changing their internet service to DSL or revoking it would be a viable solution. Perhaps fiber optic is too expensive and a reduction in price should be taken into consideration.

People without online backup, online security and device security churned significantly. Adding these services to their plan could help retain them.

People without tech support tend to churn more frequently. Adding tech support access to these customers could help prevent churn.
---

---
# Predictive Modelling
Let us now start building predictive models. A default logistic regression will be our baseline model. 75% of randomly data will be used for training. 
## Logistic Regression
---
```{r}
set.seed(100)
splits=initial_split(data,prop=0.75)
train=training(splits)
test=testing(splits)
```

```{r, echo=TRUE}


# fitting the model
fit1 = glm(Churn~.,data=train,family=binomial)
# creating predictions
pred1=predict(fit1,test,type="response")
# converting probabilities to classes; "Yes" or "No"
fit1.pred = rep("No",length(pred1))
fit1.pred[pred1 > 0.5]="Yes"
confusion_baseline = confusionMatrix(as.factor(fit1.pred),test$Churn,positive = "Yes")
confusion_baseline

## ROC Curve
library(ROCR)
pr=prediction(pred1,test$Churn)
perf=performance(pr,measure = "tpr",x.measure="fpr")
plot(perf)

## AUC
auc = performance(pr,measure="auc")
auc = auc@y.values[[1]]
auc
```
---
Our baseline performs fairly well with an accuracy of 81%. A model biased towards the majority class would result in a 73% accuracy.
We observe a hit rate (tpr) of 56%.

The ROC shows that our model is out performing random guessing which is good. An AUC value of 0.85 is considerably good.


Let us now eliminate potential multi collinearity problems by removing insignificant predictors.
---
```{r}
summary(fit1)

modcoef=summary(fit1)[['coefficients']]
modcoef[order(modcoef[ ,4]), ]

fit2 = glm(Churn~SeniorCitizen + tenure + MultipleLines + InternetService + StreamingTV + Contract + PaperlessBilling + PaymentMethod + TotalCharges
           , data=train, 
           family=binomial)
           
# making predictions 
churn.probs = predict(fit2, test, type="response")

# converting probabilities to classes; "Yes" or "No"
glm.pred = rep("No", length(churn.probs))
glm.pred[churn.probs > 0.5] = "Yes"

confusion_lr=confusionMatrix(as.factor(glm.pred), test$Churn, positive = "Yes")

# ROC AND AUC

pr = prediction(churn.probs, test$Churn)

# plotting ROC curve
prf = performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)

# AUC value
auc = performance(pr, measure = "auc")
auc = auc@y.values[[1]]
auc
```
---
The performance of the model with signnificant variables remains virtually unchanged in terms of accuracy, sensitivity and AUC
---

```{r}
new_train = train[,c('SeniorCitizen', 'tenure', 'MultipleLines', 'InternetService' , 'StreamingTV' , 'Contract' , 'PaperlessBilling' , 'PaymentMethod','TotalCharges','Churn')]

new_test= test[,c('SeniorCitizen', 'tenure', 'MultipleLines', 'InternetService' , 'StreamingTV' , 'Contract' , 'PaperlessBilling' , 'PaymentMethod','TotalCharges','Churn')]
```

---
Let us check out the total charges column and determine if using a log transformation would yield better results using correlation metric.
---
```{r, echo=TRUE}
train %>%
  select(Churn, TotalCharges) %>%
  mutate(
    Churn = Churn %>% as.factor() %>% as.numeric(),
    LogTotalCharges = log(TotalCharges)
  ) %>%
  correlate() %>%
  focus(Churn) %>%
  fashion()
```
---
It looks like log transform would definitely help making this predictor more valuable.
---
```{r}
new_train$TotalCharges = log(new_train$TotalCharges)
new_test$TotalCharges = log(new_test$TotalCharges)
```

---
We notice that there is an imbalance between the number of people who churned and the number of people who stayed ( majority staying ofcourse). This would cause problems as ML models would generalize towards the majority class and dismiss people who churned as outliers. This would lead to high type1 and type2 errors. 

Let us now try a method to deal with the class imbalance in order to optimize type 1 and type 2 errors. Oversampling synthetically generates instances of majority classes whereas undersampling removes instances of the majority class.
In most cases oversampling using techniques like SMOTE and MSMOTE will outperform conventional oversampling and undersampling methods.
---
```{r}
library(ROSE)
library(mlr)

train_over=SMOTE(Churn ~ ., data=new_train, perc.over = 270, perc.under = 150)
fit3 = glm(Churn~.,data=train_over,family=binomial)

# making predictions 
churn.probs = predict(fit3, new_test, type="response")

# converting probabilities to classes; "Yes" or "No"
glm.pred = rep("No", length(churn.probs))
glm.pred[churn.probs > 0.5] = "Yes"

confusion_lr_ov=confusionMatrix(as.factor(glm.pred), new_test$Churn, positive = "Yes")
confusion_lr_ov
# ROC

pr = prediction(churn.probs, new_test$Churn)

# plotting ROC curve
prf = ROCR::performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)

# AUC value
auc = ROCR::performance(pr, measure = "auc")
auc = auc@y.values[[1]]
auc
```

---
Before we proceed further with more models, let us evaluate the results of our logistic regression with and without SMOTE w.r.t 
what we want to achieve for our model.

## Deciding on a model measurement metric.

An overall "cost per customer measure would be apt for evaluating the effectiveness rather than accuracy or type1/tyle2 metrics. What do we mean by this? We know that classification models result in a confusion matrix when applied on a testing dataset. Each quadrant of this matrix is associated with a certain cost. Let us assume that the cost of retention of a customer is around $50. From a bit of googling, I found out that cost of acquisition is 4-5 times more than retention cost in the telecom industry. Hence, acquisition costs $250 per customer in this case. Here is the respective cost associated with each quadrant of the confusion matrix per customer

1) True positive(hit rate) : Model predicts churners correctly = $50 (retention cost)
2) False Positive(false alarm) : Model incorrectly predicts people who would churn = $50 (retention cost)
3) True Negative : Model correctly predicts customer who would stay = $0 (no cost)
4) False Negative : Model incorrectly predicts that customers would stay, meaning that a retention incentive would not be 
provided, and they end up leaving = $250 ( new customer acquisition)

Let us compute the expected cost of random guessing on the testing set
---
```{r}
summary(test$Churn)
Expected_cost_per_customer_rand = (223*50 + 222*50 + 656*250 + 657*0)/1758
Expected_cost_per_customer_rand
# $105.94
```

---
Cost per customer ona model build by random guessing amounts to $105

It is easy to infer that we desire a model with minimal false negatives(misclassification of 
actual churners) in order to optimize costs efficiently.In this case false negatives are more expensive than false posutives

Let us compare the above models with respect to this utility.
---
```{r}
confusion_lr
cost_lr_per_cust = (260*50 + 133*50 + 1180*0 + 185*250)/1758
# $37.48 per customer
confusion_lr_ov
cost_lr_ov_per_cust = (369*50 + 407*50 + 906*0 +76*250)/1758
cost_lr_ov_per_cust
# 32.87 per customer
```

---
The accuracy of the new logistic regression model reduces considerably to 72.5% compared to the baseline.However, we see a satisfactory increase in the true negatice rate.
We notice that the baseline model mitigates cost per customer from $105.94 to $37.48. The logistic regression model applied on
the balanced dataset reduces the cost further to $32.87 per customer.
With respect to the desired business requirement, modelling on the over sampled dataset is far more effective than the original dataset.

Let us now build more sophisticated models and try to optimize these costs further.


## Tuned Random Forest with kfold
---
```{r}
set.seed(10)
control = trainControl(method="repeatedcv", number=10, repeats=3, search="grid",classProbs = TRUE,summaryFunction = twoClassSummary)

mtry = sqrt(ncol(train_over))

tunegrid = expand.grid(.mtry=c(1:15))
rf_gridsearch = caret::train(Churn~., data=train_over, method="rf", metric="ROC", tuneGrid=tunegrid, trControl=control)
rf_gridsearch                 
# making predictions                  
churn.probs = predict(rf_gridsearch, new_test,type="prob")

# Yes or no
rf.pred = predict(rf_gridsearch,new_test)


# confusion matrix
confusion_rf_pro=confusionMatrix(as.factor(rf.pred), new_test$Churn, positive = "Yes")
confusion_rf_pro

#utility cost of this model
cost_per_customer_rf_pro = (307*50 + 364*50 + 138*250 + 949*0)/1758
cost_per_customer_rf_pro
#$38.7 per customer

```
---
 Random forest model has achieved a lesser accuracy of 69% and a significant decrease in sensitivity. The resulting expected value is cost of $39 per customer. 

Let us train an SVM model for this task.We will be using a linear Kernerl.

## Grid Search SVM with 10 fold cv
---
```{r}
library(dplyr)        
library(kernlab)

                  
svm_train = train_over
svm_train$Churn = ifelse(svm_train$Churn=="Yes", 1, 0)
svm_test = new_test
svm_test$Churn = ifelse(svm_test$Churn=="Yes", 1, 0)

# Preprocessing for SVM
---------------
rec_obj = recipe(Churn ~ ., data = train_over) %>%
  step_discretize(tenure, options = list(cuts = 6)) %>%
  step_log(TotalCharges) %>%  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_center(all_predictors(), -all_outcomes()) %>%
  step_scale(all_predictors(), -all_outcomes()) %>%
  prep(data = train_tbl)

train2=bake(rec_obj,newdata = train_over) 
test2=bake(rec_obj,newdata=new_test)
train2 = train2 %>% mutate(Churn = ifelse(Churn == "No", 0 ,1))
test2 = test2 %>% mutate(Churn = ifelse(Churn == "No", 0 ,1))
---------------
ytrain2 = train2$Churn
names(ytrain2)=c("Churn")
train22= train2[,-c(2)]
ytest2 = test2$Churn
test22= test2[,-c(2)]
names(ytest2)=c("Churn")

```

```{r}
grid = expand.grid( C = c(0.75, 0.9, 1, 1.1, 1.25))

ctrl = trainControl(method="repeatedcv",  
                     repeats=5,		    
                     summaryFunction=twoClassSummary,	
                     classProbs=TRUE)

svm.tune2 = caret::train(x = train2,
                    y = as.factor(ytrain2),
                    method = "svmLinear",
                    preProc = c("center","scale"),
                    metric="ROC",
                    tuneGrid = grid,
                    trControl=ctrl)
                    

# making predictions                  
churn.probs = predict(svm.tune2, test22,type="prob")
###

# Yes or no
svm.pred = predict(svm.tune2,test22)

ytest2[ytest2== 1] <- "Yes"
ytest2[ytest2 == 0] <- "No"
# confusion matrix
confusion_svm_pro2=confusionMatrix(as.factor(svm.pred), as.factor(ytest2), positive = "Yes")
confusion_svm_pro2

ytest2[ytest2== "Yes"] <- 1
ytest2[ytest2 == "No"] <- 0
pr = ROCR::prediction(churn.probs$Yes, ytest2)
# plotting ROC curve
prf = ROCR::performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)

# AUC value
auc = ROCR::performance(pr, measure = "auc")
auc = auc@y.values[[1]]
auc

#utility cost of this model
cost_per_customer_svm_pro2 = (378*50 + 426*50 + 67*250 + 887*0)/1758
cost_per_customer_svm_pro2
#$32.39 per customer    
              
```


---
SVM with linear kernel provides the best utility with $32.39 per customer. It' performance is more or less, the same compared to random forest model. However, the sensitivity of the SVM model is far superior at 84% at the cost of 4.5% reduction in specificity. The negative predictive power is very good at 93%. 

# Model assesment
We have chosen SVM as our most optimal model.
Let us now evaluate model performance w.r.t different classification thresholds and see if we can improve our model further.
---

```{r}
confusion_svm_pro2

thresh = seq(0.1,1.0, length = 10)
cost = rep(0,length(thresh))
for (i in 1:length(thresh)){
svm_pred = (rep("No", nrow(churn.probs)))
  svm_pred[churn.probs$Yes > thresh[i]] = "Yes"
  x <- confusionMatrix(as.factor(svm_pred), as.factor(ytest2), positive = "Yes")
  TN <- x$table[1]/1758
  FP <- x$table[2]/1758
  FN <- x$table[3]/1758
  TP <- x$table[4]/1758
  cost[i] = FN*250 + TP*50 + FP*50 + TN*0
}


#### BACK TO SIMPLE BASELINE MODEL WITH THRESHOLD 0.5 (logistic regression)

# fitting the model
fit1 = glm(Churn~.,data=train,family=binomial)
# creating predictions
pred1=predict(fit1,test,type ="response" )
# converting probabilities to classes; "Yes" or "No"
fit1.pred = rep("No",length(pred1))
fit1.pred[pred1 > 0.5]="Yes"
confusion_baseline = confusionMatrix(as.factor(fit1.pred),test$Churn,positive = "Yes")
baseline_cost_per_customer = (267*50 + 140*50 + 178*250 + 1173*0)/1758
baseline_cost_per_customer


## COMPUTING RESULTS INTO A DATAFRAME AND PLOTTING
dat <- data.frame(
  model = c(rep("optimized",10),"baseline"),
  cost_thresh = c(cost,baseline_cost_per_customer),
  thresh_plot = c(thresh,0.5)
)

ggplot(dat, aes(x = thresh_plot, y = cost_thresh, group = model, colour = model)) +
  geom_line() + 
  geom_point()


# cost savings of optimized model with baseline model

savings_per_customer = baseline_cost_per_customer - min(cost)
savings_per_customer


```

---
Looks like 0.5 is the best threshold for our chosen model.

Our optimal model saves us $4.49 USD per customer when compared to the baseline.
Considering a customer base of 1 million , retention strategies with inputs from this model could potentially save $4,493,000 
---

