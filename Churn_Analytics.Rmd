---
title: "Churn_analytics"
output: 
  html_document:
    keep_md: true
---

## R Markdown
---
In this project we will be doing exploratory data analysis and build high performance models to predict customer churn.
Let us start off by importing required libraries and reading in our data.
---
```{r}
library(tidyverse)
library(tidyquant)
library(rsample)
library(recipes)
library(corrr)
library(reshape2)
library(ggplot2)
library(Amelia)
library(ggpubr)
library(caret)
library(ROCR)
library(randomForest)
library(mlbench)
library(caret)
library(DMwR)
data=read.csv(file.choose())
```
---
Initial glimpse
---

```{r, echo=TRUE}
dim(data)
names(data)
```

```{r, echo=TRUE}
head(data,5)
```


```{r, echo=TRUE}
sapply(data,class)
```


```{r, echo=TRUE}
missmap(data)
```

---
Looks like there are very few missing values in the total purchases column. Lets start pre-processing the data by removing rows with these values and the customer ID column. We will also convert senior citizen and character columns to factor.
---

```{r, echo=TRUE}
data = data %>%
  select(-customerID) %>%
  drop_na() %>%
  select(Churn, everything())

data = data %>% mutate_if(is.character, as.factor)
data$SeniorCitizen = as.factor(data$SeniorCitizen) 

```
---
# Exploratory Data Analysis
We are ready to start exploring. Lets see how various columns play a role in churn rates. We'll start with gender.
---
```{r, echo=TRUE}
data %>%
  group_by(gender) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n))
```


```{r, echo=TRUE}
data %>%
  group_by(gender, Churn) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n))
```


---
It doesnt look like gender plays a meaningful role. The churn rates are more or less the same. Let us now have a look at senior citizen variable.
---

```{r, echo=TRUE}
data %>%
  group_by(SeniorCitizen) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n))
```

```{r, echo=TRUE}
data %>%
  group_by(SeniorCitizen, Churn) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n))
```

---
Out of the 16% of the customers who are senior citizens almost 42% churned. 24% churn rate can be observed in the remaining population. We can infer than senior citizens are more likely to churn.
---

```{r, echo=TRUE}
data %>%
  group_by(Partner) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n))
```

```{r, echo=TRUE}
data %>%
  group_by(Partner, Churn) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n))
```

---
People with partners have 20% churn rate. On the other hand 33% churn rate can be observed among people without partners.
---
```{r, echo=TRUE}
data %>%
  group_by(Dependents) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n))
```

```{r, echo=TRUE}
data %>%
  group_by(Dependents, Churn) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n))
```

---
15% churn rate can be observed among people who have dependents. Among those who do not have dependants, we can see a 30% churn rate. It appears that people who do not have dependents are twice as likely to churn.

Among the variables we observed earlier, we consider subsets where higher churn rate is observed i.e, senior citizen, people without dependants and people without partners. Let us observed the total charges of customers in these segments who have churned.
---
```{r, echo=TRUE}
data %>%
  select(SeniorCitizen, Churn, TotalCharges, tenure) %>%
  filter(SeniorCitizen == 1, Churn == "Yes") %>%
  summarize(n = n(),
            total = sum(TotalCharges),
            avg_tenure = sum(tenure)/n)
```


```{r, echo=TRUE}
data %>%
  select(Partner, Churn, TotalCharges, tenure) %>%
  filter(Partner == "No", Churn == "Yes") %>%
  summarize(n = n(),
            total = sum(TotalCharges),
            avg_tenure = sum(tenure)/n)
```


```{r, echo=TRUE}
data %>%
  select(Dependents, Churn, TotalCharges, tenure) %>%
  filter(Dependents == "No", Churn == "Yes") %>%
  summarize(n = n(),
            total = sum(TotalCharges),
            avg_tenure = sum(tenure)/n)
```

---
Among the segments, the highest loss of 2.3 million USD can be observed in people without dependents. People without partners churned 1.3 million USD while senior citizens churned 0.9 million USD.

Let us observed the services used by the segment of people without dependents
---
```{r}
dependents = data %>% filter(Dependents == "No")
p1=ggplot(dependents,aes(x=PhoneService,fill=Churn))+geom_bar()+geom_text(stat='count',aes(label=..count..))
p2=ggplot(dependents,aes(x=MultipleLines,fill=Churn))+geom_bar()+geom_text(stat='count',aes(label=..count..))
p3=ggplot(dependents,aes(x=InternetService,fill=Churn))+geom_bar()+geom_text(stat='count',aes(label=..count..))
p4=ggplot(dependents,aes(x=OnlineSecurity,fill=Churn))+geom_bar()+geom_text(stat='count',aes(label=..count..))
p5=ggplot(dependents,aes(x=OnlineBackup,fill=Churn))+geom_bar()+geom_text(stat='count',aes(label=..count..))
p6=ggplot(dependents,aes(x=DeviceProtection,fill=Churn))+geom_bar()+geom_text(stat='count',aes(label=..count..))
p7=ggplot(dependents,aes(x=TechSupport,fill=Churn))+geom_bar()+geom_text(stat='count',aes(label=..count..))
p8=ggplot(dependents,aes(x=StreamingTV,fill=Churn))+geom_bar()+geom_text(stat='count',aes(label=..count..))
p9=ggplot(dependents,aes(x=StreamingMovies,fill=Churn))+geom_bar()+geom_text(stat='count',aes(label=..count..))
p10=ggplot(dependents,aes(x=Contract,fill=Churn))+geom_bar()+geom_text(stat='count',aes(label=..count..))
p11=ggplot(dependents,aes(x=PaperlessBilling,fill=Churn))+geom_bar()+geom_text(stat='count',aes(label=..count..))

```


```{r, echo=TRUE}
ggarrange(p1,p2,p3,p4, ncol=2,nrow=2)
```


```{r, echo=TRUE}
ggarrange(p5, p6 , p7 ,p8, ncol=2,nrow=2)
```


```{r, echo=TRUE}
ggarrange(p9,p10,p11, ncol=2,nrow=2)
```

---
High churn rates can be observed in people using phone service. Maybe they do not use the service often. Removing phone service from their plan might mitigate their expenses and improve retention.

People who use fiber optic interned have churned significantly. Perhaps changing their internet service to DSL or revoking it would be a viable solution. Perhaps fiber optic is too expensive and a reduction in price should be taken into consideration.

People without online backup, online security and device security churned significantly. Adding these services to their plan could help retain them.

People without tech support tend to churn more frequently. Adding tech support access to these customers could help prevent churn.
---

---
# Predictive Modelling
Let us now start building predictive models. A default logistic regression will be our baseline model. 75% of randomly data will be used for training. 
## Logistic Regression
---
```{r}
set.seed(100)
splits=initial_split(data,prop=0.75)
train=training(splits)
test=testing(splits)
```

```{r, echo=TRUE}
# fitting the model
fit1 = glm(Churn~.,data=train,family=binomial)
# creating predictions
pred1=predict(fit1,test,type="response")
# converting probabilities to classes; "Yes" or "No"
fit1.pred = rep("No",length(pred1))
fit1.pred[pred1 > 0.5]="Yes"
confusionMatrix(as.factor(fit1.pred),test$Churn,positive = "Yes")
```

---
Right off the bat, our model gave almost 82% accuracy on our training set. However, accuracy isn't a viable metric for our use case. Sensitivity is a much better metric in this case. Sensitivity or true positive rate defines how accurately our model predicts customers who actually churned. Maximizing this metric should be the top priority. Minimizing misclassification of people who actually churn will lead to better strategic decision making and save the company a lot of money.

In this model the sensitivity is found to be 60%. This means the model accurately predicted customers who would churn 60% of the time.
---
```{r, echo=TRUE}
predobj=prediction(pred1,test$Churn)
#Check out auc
pobj=prediction(pred1,test$Churn)
auc=performance(pobj,measure="auc")
auc=auc@y.values[[1]]
auc
```

---
This model gives us an AUC of 0.85 which is good Below is a summary of our model. Do not be alarmed by the NA's in ther summary. These variables do not get considered in model computation due to having perfect collinearity with another variable. We can ignore these.
---

```{r, echo=TRUE}
summary(fit1)
```

---
For our next model we will be considering a logistic regression model with variables we found to be significant ( p < 0.05) in the summary of our previous model.
## Logistic Regression with significant variables
---

```{r, echo=TRUE}

fit2 = glm(Churn~tenure  + Contract + PaperlessBilling + PaymentMethod + TotalCharges, data=train, family=binomial)
pred2 = predict(fit2, test, type="response")

contrasts(data$Churn)  # Yes = 1, No = 0
fit2.pred = rep("No", length(pred2))
fit2.pred[pred2 > 0.5] = "Yes"

confusionMatrix(as.factor(fit2.pred), test$Churn, positive = "Yes")
```

---
We can see a slight reduction in accuracy to about 80%. What is more notable is a significant drop in sensitivity to 53%. Excluding insignificant features made our model worse. Let us review AUC metric.
---
```{r, echo=TRUE}
pr = prediction(pred2, test$Churn)
auc = ROCR::performance(pr,measure ="auc")
auc = auc@y.values[[1]]
auc
```

---
AUC more or less is similar to the previous model.

Another reason for a low sensitivity with out models is due to the problem of imbalanced classes. From our initial exploration, we can see that people who did not churn severely outnumber churners by about 2.7 times. This maybe good news for the company, but bad news for classification modelling. Due to this, machine learning models generalize towards the majority class and tend to dismiss the minority class as outliers. Though the model may get a decent or even an extremely good overall accuracy, it would most likely would have a low classification accuracy on the minority class. 

A method to overcome this is oversampling. We will be using an oversampling algorithm called SMOTE to synthetically create samples on our minority class. Before that let us pre process this data further in order to use SMOTE.
---

---
Let us check out the total charges column and determine if using a log transformation would yield better results using correlation metric.
---
```{r, echo=TRUE}
train %>%
  select(Churn, TotalCharges) %>%
  mutate(
    Churn = Churn %>% as.factor() %>% as.numeric(),
    LogTotalCharges = log(TotalCharges)
  ) %>%
  correlate() %>%
  focus(Churn) %>%
  fashion()
```


---
It looks like log transform would definitely help making this predictor more valuable.

Now in the next chunk of code does the following:
1) cut number of years in tenure into 6 segments.
2) log transform total charges
3) one hot encoding on categorical data
4) mean- center the data
5) scale the data
---
```{r, echo=TRUE}
rec_obj = recipe(Churn ~ ., data = train) %>%
  step_discretize(tenure, options = list(cuts = 6)) %>%
  step_log(TotalCharges) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_center(all_predictors(), -all_outcomes()) %>%
  step_scale(all_predictors(), -all_outcomes()) %>%
  prep(data = train_tbl)
```

```{r}
train2=bake(rec_obj,newdata = train) 
test2=bake(rec_obj,newdata=test)
train2 = train2 %>% mutate(Churn = ifelse(Churn == "No", 0 ,1))
test2 = test2 %>% mutate(Churn = ifelse(Churn == "No", 0 ,1))

```
---
SMOTE will be applied on the training set.
---

```{r}
## for debugging purposes
train2$Churn=as.factor(train2$Churn)
test2$Churn=as.factor(test2$Churn)
conv=as.data.frame(train2)
test2=as.data.frame(test2)
## apply smote
new_train=SMOTE(Churn ~ ., data=conv, perc.over = 270, perc.under = 150)
```

---
## Logistic Regression on SMOTE-d data
Let us now apply logistic regression on this new training data and evaluate its performance.
---

```{r, echo=TRUE}
fit3 = glm(Churn~.,data=new_train,family=binomial)
pred3=predict(fit3,test2)
pred3=ifelse(pred3 > 0.5,1,0)
confusionMatrix(as.factor(pred3),as.factor(test2$Churn))
```

---
The accuracy is more, or less the same as previous models. However, we can see a drastic improvement in the sensitivity to almost 83%. Specitivity ( classification rate of non- churners) reduced to 72% but this can be overlooked considering the business problem. So far, this is clearly the best model.
---

```{r, echo=TRUE}
predobj=prediction(pred3,test2$Churn)
pobj=ROCR::prediction(pred3,test2$Churn)
auc=ROCR::performance(pobj,measure="auc")
auc=auc@y.values[[1]]
auc
```

---
AUC has dropped to 77% but the tradeoff is still valuable considering the results form the confusion matrix above.
---

---
## Tuned Random Forest in SMOTE data.
We will now upscale our model to that of random forests. Parameters will be fine tuned via grid search and 10 fold cross validation will be applied for training.
---
```{r}
#for debugging purposes
levels(new_train$Churn) <- make.names(levels(factor(new_train$Churn)))
levels(test2$Churn) <- make.names(levels(factor(test2$Churn)))
```

```{r, echo=TRUE}
control = trainControl(method="repeatedcv", number=10, repeats=3, search="grid",classProbs = TRUE,summaryFunction = twoClassSummary)
mtry = sqrt(ncol(new_train))
tunegrid = expand.grid(.mtry=c(1:15))
rf_gridsearch = caret::train(Churn~., data=new_train, method="rf", metric="ROC", tuneGrid=tunegrid, trControl=control)
print(rf_gridsearch)
```


---
We get an ROC of 96.9% and sensitivity of 92.8%. Let us see how this performs on testing data set.
---
```{r, echo=TRUE}
plot(rf_gridsearch)
```



```{r, echo=TRUE}
rfpro_predict=predict(rf_gridsearch,test2)
confusionMatrix(rfpro_predict,test2$Churn)

```


---
There isn't much of a significant change from our last model. Compared to the results from 10 fold cv, we can infer presence of over fitting.

Let us now evaluate by AUC value.
---
```{r, echo=TRUE}
rfpro.predict.prob = predict(rf_gridsearch, test2, type="prob")
pobj=prediction(rfpro.predict.prob[,2],test2$Churn)
auc=ROCR::performance(pobj,measure="auc")
auc=auc@y.values[[1]]
auc
```

---
However based on an improved AUC score of about 83%, this appears to be a more reliable model.
---